{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8135713-846c-4666-a6f7-2c0b4922a131",
   "metadata": {},
   "source": [
    "# TEMPO UVAI vs DSCOVR (spatial)\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook illustrates a comparison of TEMPO ultra-violet aerosol index (UVAI) against DSCOVR EPIC UVAI. TEMPO_O3TOT_L2_V03 and DSCOVR_EPIC_L2_AER_03 are the data collections used as sources of UVAI.\n",
    "\n",
    "TEMPO and DSCOVR granules are downloaded on-the-fly with [earthaccess](https://earthaccess.readthedocs.io/en/latest/) library, which may need to be installed first.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "\"DSCOVR_EPIC_L2_AER_03 is the Deep Space Climate Observatory (DSCOVR) Enhanced Polychromatic Imaging Camera (EPIC) Level 2 UV Aerosol Version 3 data product. Observations for this data product are at 340 and 388 nm and are used to derive near UV (ultraviolet) aerosol properties. The EPIC aerosol retrieval algorithm (EPICAERUV) uses a set of aerosol models to account for the presence of carbonaceous aerosols from biomass burning and wildfires (BIO), desert dust (DST), and sulfate-based (SLF) aerosols. These aerosol models are identical to those assumed in the OMI (Ozone Monitoring Instrument) algorithm (Torres et al., 2007; Jethva and Torres, 2011).\" ([Source](https://asdc.larc.nasa.gov/project/DSCOVR/DSCOVR_EPIC_L2_AER_03))\n",
    "\n",
    "Total ozone Level 2 files provide ozone information at Tropospheric Emissions: Monitoring of Pollution (TEMPO)â€™s native spatial resolution, ~10 km^2 at the center of the Field of Regard (FOR), for individual granules. Each granule covers the entire North-South TEMPO FOR but only a portion of the East-West FOR.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "A free(!) account at https://www.earthdata.nasa.gov/ is needed to login and download the appropriate files.  You will use is during the \"Establish access to Earthdata\" Section.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup\n",
    "2. Define utility functions for DSCOVR and TEMPO data\n",
    "3. Establish access to Earthdata\n",
    "4. Select timeframe of interest\n",
    "5. Retrieving DSCOVR EPIC granules\n",
    "6. For every DSCOVR EPIC granule, find simultaneous TEMPO granules and re-map DSCOVR EPIC data to geolocations of TEMPO\n",
    "\n",
    "## Notebook's general code outline:\n",
    "- Timeframe of interest is selected by a user.\n",
    "- Searches for DSCOVR EPIC granules withing the TEMPO field of regard (FOR) and within user's timeframe by means of earthaccess library.\n",
    "- After downloading DSCOVR EPIC granules, a loop by these granules DSCOVR L2 AER data searches for TEMPO granules simultaneous with DSCOVR EPIC one.\n",
    "- If such TEMPO granules exist, DSCOVR EPIC UVAI retroevals are interpolated to the positions of the TEMPO pixels. The interpolated values are ritten into a netCDF file along with TEMPO geolocations.\n",
    "- Finally original UVAI from DSCOVR EPIC and TEMPO are plotted along with interpolated DSCOVR EPIC values in the same plot. Output images are written into PNG files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5a63b-bf77-4d3f-bbb4-781f2d63e6a0",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc6e7df-e8ca-4f89-8d89-7b600c7d4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta # needed to work with time in plotting time series\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import earthaccess # needed to discover and download TEMPO data\n",
    "import h5py # needed to read DSCOVR_EPIC_L2_TO3 files\n",
    "import matplotlib.pyplot as plt # needed to plot the resulting time series\n",
    "import netCDF4 as nc # needed to read TEMPO data\n",
    "import numpy as np\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from shapely.geometry import Point, Polygon # needed to search a point within a polygon\n",
    "from scipy.interpolate import griddata # needed to interpolate TEMPO data to the point of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72213f01-0911-4447-a9f8-b59a74bbb24a",
   "metadata": {},
   "source": [
    "# 2. Define utility functions for DSCOVR and TEMPO data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee6a50-837f-45cf-952a-deb992d367b1",
   "metadata": {},
   "source": [
    "## 2.1 Function to read DSCOVR AER data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790e6c6-4a5c-431b-8046-f4c25da07ec4",
   "metadata": {},
   "source": [
    "Function `read_epic_l2_AER` reads a DSCOVR_EPIC_L2_AER product file given by its filename\n",
    "and returns arrays of 2D latitudes, longitudes, UVAI, AOD, and wavelength,\n",
    "along with their fill values and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5addad8-7357-4b4e-b283-d8cb55436c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_array_and_fill_value(file_object: h5py.File, dataset_path: str):\n",
    "        h5_dataset = file_object[dataset_path]\n",
    "        return np.array(h5_dataset[:]), h5_dataset.fillvalue\n",
    "\n",
    "def read_epic_l2_AER(filename: str):\n",
    "\n",
    "    aod_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields/FinalAerosolOpticalDepth'\n",
    "    uvai_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields/UVAerosolIndex'\n",
    "    lat_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Geolocation Fields/Latitude'\n",
    "    lon_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Geolocation Fields/Longitude'\n",
    "    wl_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields/Wavelength'\n",
    "\n",
    "    arrays = {}\n",
    "    fill_values = {}\n",
    "      \n",
    "    with h5py.File(filename, \"r\" ) as f:\n",
    "        arrays[\"aod2D\"], fill_values[\"aod\"] = get_dataset_array_and_fill_value(f, aod_name)\n",
    "        arrays[\"uvai2D\"], fill_values[\"uvai\"] = get_dataset_array_and_fill_value(f, uvai_name)\n",
    "        arrays[\"lat2D\"], fill_values[\"lat\"] = get_dataset_array_and_fill_value(f, lat_name)\n",
    "        arrays[\"lon2D\"], fill_values[\"lon\"] = get_dataset_array_and_fill_value(f, lon_name)\n",
    "        arrays[\"wl\"], fill_values[\"wl\"] = get_dataset_array_and_fill_value(f, wl_name)\n",
    "    \n",
    "    # Get time from the granule's filename.\n",
    "    timestamp = datetime.strptime(filename.split('_')[-2], '%Y%m%d%H%M%S')\n",
    "\n",
    "    return arrays, fill_values, timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2164a-801c-441c-9923-2251e8f83465",
   "metadata": {},
   "source": [
    "## 2.2 Function to read UV Aerosol Index from TEMPO O3TOT data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9745a0b2-1d21-4e34-bef2-7e7efe0aa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_TEMPO_O3TOT_L2_UVAI(filename):\n",
    "    \"\"\"Read the following product arrays from the TEMPO_O3TOT_L2_V01(2):\n",
    "        - vertical_column\n",
    "        - vertical_column_uncertainty\n",
    "        \n",
    "    and returns the respective fields along with coordinates of the pixels.\n",
    "    \"\"\"\n",
    "    var_name = 'uv_aerosol_index'\n",
    "    var_QF_name = 'quality_flag'\n",
    "\n",
    "    arrays = {}\n",
    "    fill_values = {}\n",
    "    \n",
    "    with nc.Dataset(filename) as ds:\n",
    "        # Open the product group (/product), read the chosen UVAI variable and its quality flag.\n",
    "        prod = ds.groups['product']\n",
    "        arrays[\"uvai\"] = prod.variables[var_name][:]\n",
    "        fill_values[\"uvai\"] = prod.variables[var_name].getncattr('_FillValue')\n",
    "        var_QF = prod.variables[var_QF_name]\n",
    "        arrays[\"uvai_QF\"] = var_QF[:]\n",
    "        # Note: there is no fill value for the quality flag.\n",
    "        # Once it is available in the next version of the product,\n",
    "        # un-comment the line below and add fv_QF to the return line.\n",
    "        #    fv_QF = var_QF.getncattr('_FillValue'\n",
    "        \n",
    "        # Open geolocation group (/geolocation), and \n",
    "        #   read the latitude and longitude variables into a numpy array.\n",
    "        geo = ds.groups['geolocation']\n",
    "        arrays[\"lat\"] = geo.variables['latitude'][:]\n",
    "        arrays[\"lon\"] = geo.variables['longitude'][:]\n",
    "        fill_values[\"geo\"] = geo.variables['latitude'].getncattr('_FillValue')\n",
    "        # Note: it appeared that garbage values of latitudes and longitudes in the L2 files\n",
    "        # are 9.969209968386869E36 while fill value is -1.2676506E30\n",
    "        # (after deeper search, it was found that the actual value in the file is -1.2676506002282294E30).\n",
    "        # For this reason, fv_geo is set to 9.96921E36 to make the code working.\n",
    "        # Once the problem is resolved and garbage values of latitudes and longitudes\n",
    "        # equal to their fill value, the line below must be removed.\n",
    "        fill_values[\"geo\"] = 9.969209968386869E3\n",
    "        \n",
    "        # Read the time variable from geo (geolocation group, /geolocation) into a numpy array.\n",
    "        arrays[\"time\"] = geo.variables['time'][:]\n",
    "\n",
    "    return arrays, fill_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9dbb8-eecb-4f70-aa6e-78ea084978a9",
   "metadata": {},
   "source": [
    "## 2.3 Function to create TEMPO O3 granule polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18cb4d5d-5f7b-4292-8404-98c48d5ef329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TEMPO_L2_polygon(lat, lon, geo_fillvalue):\n",
    "    nx, ny = lon.shape\n",
    "    print(f\"granule has {nx: >3} scanlines by {ny: >4} pixels\")\n",
    "    dpos = np.empty([0,2])\n",
    "    \n",
    "    # Create arrays for x and y indices.\n",
    "    x_indices = np.arange(nx).reshape(-1, 1)\n",
    "    y_indices = np.arange(ny).reshape(1, -1)\n",
    "    x_index_array = np.repeat(x_indices, ny, axis=1)\n",
    "    y_index_array = np.repeat(y_indices, nx, axis=0)\n",
    "    \n",
    "    mask = (lon[nx - 1, ny - 1] != geo_fillvalue) & (lat[nx - 1, ny - 1] != geo_fillvalue)\n",
    "    if len(lon[mask]) == 0:\n",
    "        print('the granule is empty - no meaningful positions')\n",
    "        return dpos\n",
    "\n",
    "    def get_local_mask(row_index):\n",
    "        return (lon[row_index, :] != geo_fillvalue) & (lat[row_index, :] != geo_fillvalue)\n",
    "    \n",
    "    # Right boundary.\n",
    "    right_min_index = np.min(x_index_array[mask])\n",
    "    local_mask = get_local_mask(right_min_index)\n",
    "    right_boundary = np.stack((lon[right_min_index, local_mask], lat[right_min_index, local_mask])).T\n",
    "\n",
    "    # Left boundary.\n",
    "    left_max_index = np.max(x_index_array[mask])\n",
    "    local_mask = get_local_mask(left_max_index)\n",
    "    left_boundary = np.stack((lon[left_max_index, local_mask], lat[left_max_index, local_mask])).T\n",
    "\n",
    "    # Top and bottom boundaries.\n",
    "    top_boundary = np.empty([0,2])\n",
    "    bot_boundary = np.empty([0,2])\n",
    "    for ix in range(right_min_index + 1, left_max_index):\n",
    "        local_mask = get_local_mask(ix)\n",
    "        local_y_ind = y_index_array[ix, local_mask]\n",
    "        y_ind_top = min(local_y_ind)\n",
    "        y_ind_bottom = max(local_y_ind)\n",
    "        top_boundary = np.append(top_boundary, [[lon[ix, y_ind_top], lat[ix, y_ind_top]]], axis=0)\n",
    "        bot_boundary = np.append(bot_boundary, [[lon[ix, y_ind_bottom], lat[ix, y_ind_bottom]]], axis=0)\n",
    "\n",
    "    # Combine right, top, left, and bottom boundaries together, going along the combined boundary counterclockwise.\n",
    "    dpos = np.append(dpos, right_boundary[ : :-1, :], axis=0)\n",
    "    dpos = np.append(dpos, top_boundary, axis=0)\n",
    "    dpos = np.append(dpos, left_boundary, axis=0)\n",
    "    dpos = np.append(dpos, bot_boundary[ : :-1, :], axis=0)\n",
    "    \n",
    "    print('polygon shape: ',dpos.shape)\n",
    "    \n",
    "    coords_poly = list(dpos)\n",
    "    poly = Polygon(coords_poly)\n",
    "    \n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31506cd8-eef3-4296-b46b-aa1578892cf7",
   "metadata": {},
   "source": [
    "## 2.4 Function to write DSCOVR EPIC UV Aerosol Index re-mapped to TEMPO granule locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8e9f4e-e335-4224-8da2-3aced2cf8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_DSCOVR_TEMPO_UVAI(filename, lat2D, lon2D, uvai2D):\n",
    "    \"\"\"Create a netCDF file. Return True if successful, otherwise return False.\n",
    "    \n",
    "    variables:\n",
    "        filename: TEMPO file name, which will be used to create output file name\n",
    "        lat2D: 2D array of TEMPO latitudes\n",
    "        lon2D: 2D array of TEMPO longitudes\n",
    "        uvai2D: 2D array of DSCOVR EPIC UVAI re-mapped to TEMPO locations\n",
    "        \n",
    "    Arrays above should be of the same shape.\n",
    "    \"\"\"\n",
    "    (nx, ny) = lat2D.shape\n",
    "    \n",
    "    with nc.Dataset('DSCOVR_UVAI_' + filename, mode='w', format='NETCDF4_CLASSIC') as ncf:\n",
    "        x_dim = ncf.createDimension('mirror_step', nx) # number of scanlines\n",
    "        y_dim = ncf.createDimension('xtrack', ny) # number of pixels in a scanline\n",
    "        \n",
    "        lat = ncf.createVariable('lat', np.float32, ('mirror_step', 'xtrack'))\n",
    "        lat.units = 'degrees_north'\n",
    "        lat.long_name = 'latitude'\n",
    "        lat[:,:] = lat2D\n",
    "        \n",
    "        lon = ncf.createVariable('lon', np.float32, ('mirror_step', 'xtrack'))\n",
    "        lon.units = 'degrees_east'\n",
    "        lon.long_name = 'longitude'\n",
    "        lon[:,:] = lon2D\n",
    "        \n",
    "        uv_aerosol_index = ncf.createVariable('uv_aerosol_index', np.float32, ('mirror_step', 'xtrack'))\n",
    "        uv_aerosol_index[:,:] = uvai2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2708ee-a2b0-4baa-8a0d-ad9a853e7ec5",
   "metadata": {},
   "source": [
    "## 2.5 Function to find TEMPO granules that match time of DSCOVR granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd146ab-78c2-4e87-a855-81620e67ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting TEMPO name constants\n",
    "tempo_short_name = 'TEMPO_O3TOT_L2' # collection name to search for in the EarthData\n",
    "tempo_version = 'V03' # this is the latest available version as of August 02, 2024\n",
    "\n",
    "def get_TEMPO_results_to_match_DSCOVR_granule(dscovr_timestamp):\n",
    "    \"\"\"Return results for TEMPO. If none found, return None.\n",
    "    \n",
    "    It was discovered that the actual timespan of an EPIC granule begins 289 s before\n",
    "    the granule timestamp and ends 107 s after it.\n",
    "    This timeframe will be used for search of TEMPO granules.\n",
    "    \"\"\"\n",
    "    timestamp1 = dscovr_timestamp + timedelta(seconds = -289)\n",
    "    timestamp2 = dscovr_timestamp + timedelta(seconds = 107)\n",
    "    print(dscovr_timestamp, timestamp1, timestamp2)\n",
    "\n",
    "    # Try twice.\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            results = earthaccess.search_data(short_name=tempo_short_name, \n",
    "                                              version=tempo_version, \n",
    "                                              temporal=(timestamp1, timestamp2))\n",
    "            \n",
    "            if len(results) > 0:\n",
    "                print(f\"Total number of TEMPO version {tempo_version} granules found\"\n",
    "                      f\"\\nwithin period of interest between {timestamp1} and {timestamp2}\"\n",
    "                      f\" is {len(results)}\")\n",
    "                return results\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    print(f\"Zero TEMPO version {tempo_version} granules found\"\n",
    "          f\"\\nwithin period of interest between {timestamp1} and {timestamp2}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6b7e1-8e0a-4cb5-8141-df535a5aeda5",
   "metadata": {},
   "source": [
    "## 2.6 Function to find 'good' TEMPO data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e98ee48-7200-4f74-b52a-c349047293b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_tempo_points(lat, lon, geo_fillvalue, uvai, uvai_fillvalue):\n",
    "    # Mask out fill values of TEMPO lat/lon positions.\n",
    "    mask = (lat != geo_fillvalue) & (lon != geo_fillvalue) & (uvai != uvai_fillvalue)\n",
    "    lon1D = lon[mask]\n",
    "    lat1D = lat[mask]\n",
    "    lonlat_stacked = np.column_stack((lon1D, lat1D))\n",
    "    uvai1D = uvai[mask]\n",
    "    extent = (min(lon1D), max(lon1D), min(lat1D), max(lat1D))\n",
    "\n",
    "    # Create arrays of indices to restore TEMPO 2D arrays after re-mapping.\n",
    "    nx, ny = lat.shape\n",
    "    y_index_array = np.tile(np.linspace(0, ny, ny, endpoint=False, dtype=int), (nx, 1))\n",
    "    x_index_array = np.tile(np.linspace(0, nx, nx, endpoint=False, dtype=int), (ny, 1)).transpose()\n",
    "    x_indices_masked = x_index_array[mask]\n",
    "    y_indices_masked = y_index_array[mask]\n",
    "    print(f\"nx, ny = {nx}, {ny}\")\n",
    "\n",
    "    # Restore 2D arrays by filling over a fill value.\n",
    "    lat2D = np.full((nx, ny), -999.)\n",
    "    lon2D = np.full((nx, ny), -999.)\n",
    "    uvai2D = np.full((nx, ny), -999.)\n",
    "    \n",
    "    for ix, iy, lon1, lat1, uvai1 in zip(\n",
    "            x_indices_masked, y_indices_masked, lon1D, lat1D, uvai1D\n",
    "    ):\n",
    "        lat2D[ix, iy] = lat1\n",
    "        lon2D[ix, iy] = lon1\n",
    "        uvai2D[ix, iy] = uvai1\n",
    "\n",
    "    # Write restored 2D arrays to a netCDF file.\n",
    "    try:\n",
    "        write_DSCOVR_TEMPO_UVAI(tempo_file_name, lat2D, lon2D, uvai2D)\n",
    "    except Exception:\n",
    "        print('failed to write restored TEMPO 2D arrays into the output file')\n",
    "\n",
    "    return lonlat_stacked, extent, lon1D, lat1D, uvai1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195d26a-8de9-4ec2-a962-66f3869a820f",
   "metadata": {},
   "source": [
    "## 2.7 Function to determine DSCOVR data points within the domain of TEMPO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e116eef-6462-48fd-bbcc-ee9ffdd7ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_dscovr_to_tempo(lat2D, lon2D, uvai2D, uvai_fillvalue, tempo_extent, tempo_polygon) -> dict | None:\n",
    "    # Mask out DSCOVR UVAI to the geospatial extent of TEMPO granule.\n",
    "    mask = (\n",
    "        (uvai2D != uvai_fillvalue) &\n",
    "        (lat2D > tempo_extent[2]) & (lat2D < tempo_extent[3]) &\n",
    "        (lon2D > tempo_extent[0]) & (lon2D < tempo_extent[1])\n",
    "    )\n",
    "    lon1D = lon2D[mask]\n",
    "    lat1D = lat2D[mask]\n",
    "    uvai1D = uvai2D[mask]\n",
    "\n",
    "    # Number of DSCOVR pixels falling into ranges min_TEMPO_lat < lat2D < max_TEMPO_lat, min_TEMPO_lon < lat2D < max_TEMPO_lon.\n",
    "    n_DSCOVR_TEMPO = len(uvai1D)\n",
    "    if n_DSCOVR_TEMPO == 0:\n",
    "        print('no original DSCOVR pixels within TEMPO granule')\n",
    "        return None\n",
    "\n",
    "    mask_DSCOVR_in_TEMPO = np.empty(n_DSCOVR_TEMPO, dtype=np.bool_)\n",
    "    for i in range(n_DSCOVR_TEMPO):\n",
    "        dscovr_point = Point(np.array([lon1D[i], lat1D[i]]))\n",
    "        mask_DSCOVR_in_TEMPO[i] = dscovr_point.within(tempo_polygon)\n",
    "\n",
    "    lon1D_DSCOVR_TEMPO = lon1D[mask_DSCOVR_in_TEMPO]\n",
    "    lat1D_DSCOVR_TEMPO = lat1D[mask_DSCOVR_in_TEMPO]\n",
    "    uvai1D_DSCOVR_TEMPO = uvai1D[mask_DSCOVR_in_TEMPO]\n",
    "\n",
    "    return {\n",
    "        \"lon1D\": lon1D_DSCOVR_TEMPO,\n",
    "        \"lat1D\": lat1D_DSCOVR_TEMPO,\n",
    "        \"uvai1D\": uvai1D_DSCOVR_TEMPO,\n",
    "        \"mask\": mask_DSCOVR_in_TEMPO\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812bd5c-17c9-4f35-a7bc-5c4b7b15cb80",
   "metadata": {},
   "source": [
    "# 3. Establish access to EarthData - Log in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a196528-e7ea-43c6-b971-ff4294cd5730",
   "metadata": {},
   "source": [
    "Function `earthaccess.login` prompts for EarthData login and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18af31d6-5bbe-4d63-8ffd-0d6f00ae1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(strategy=\"interactive\", persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789afe8-ec9b-4b36-a857-ac997bacc04d",
   "metadata": {},
   "source": [
    "# 4. Select timeframe of interest\n",
    "\n",
    "DSCOVR EPIC granules will be searched within this timeframe.\n",
    "\n",
    "For the tutorial demonstration, `20230805` was used for both the start and end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c731ad67-ebe4-44fd-8e70-973701edd6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter period of interest, start and end dates, in the form YYYYMMDD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter start date of interest  20230805\n",
      "enter end date of interest  20230805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 8 5 2023 8 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter start date of interest  20230805\n",
      "enter end date of interest  20230805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 8 5 2023 8 5\n"
     ]
    }
   ],
   "source": [
    "print('enter period of interest, start and end dates, in the form YYYYMMDD')\n",
    "start_date = int(input('enter start date of interest '))\n",
    "end_date = int(input('enter end date of interest '))\n",
    "\n",
    "yyyy_ini = start_date//10000\n",
    "mm_ini = (start_date//100 - yyyy_ini*100)\n",
    "dd_ini = (start_date - yyyy_ini*10000 - mm_ini*100)\n",
    "\n",
    "yyyy_fin = end_date//10000\n",
    "mm_fin = (end_date//100 - yyyy_fin*100)\n",
    "dd_fin = (end_date - yyyy_fin*10000 - mm_fin*100)\n",
    "print(yyyy_ini, mm_ini, dd_ini, yyyy_fin, mm_fin, dd_fin)\n",
    "\n",
    "date_start = str('%4.4i-%2.2i-%2.2i 00:00:00' %(yyyy_ini, mm_ini, dd_ini))\n",
    "date_end = str('%4.4i-%2.2i-%2.2i 23:59:59' %(yyyy_fin, mm_fin, dd_fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a54f7-6da7-49ea-ba7e-3f00b31546a1",
   "metadata": {},
   "source": [
    "# 5. Retrieve DSCOVR EPIC granules\n",
    "\n",
    "We search for DSCOVR EPIC granules that fall within the time of interest and within the TEMPO Field of Regard (FOR) polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a36a24d-a242-483a-9849-2eb7e9edf268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of DSCOVR EPIC L2_AER granules found for TEMPO Field of Regard\n",
      "within period of interest between 2023-08-05 00:00:00 and 2023-08-05 23:59:59 is 21\n"
     ]
    }
   ],
   "source": [
    "short_name = 'DSCOVR_EPIC_L2_AER' # collection name to search for in the EarthData\n",
    "\n",
    "# The polygon below is taken from MMT description of TEMPO_O3TOT_L2,\n",
    "# see https://mmt.earthdata.nasa.gov/collections/C2842849465-LARC_CLOUD\n",
    "# Polygon: (10.0Â°, -170.0Â°), (10.0Â°, -10.0Â°), (80.0Â°, -10.0Â°), (80.0Â°, -170.0Â°), (10.0Â°, -170.0Â°)\n",
    "\n",
    "bbox = (-170., 10., -10., 80.)\n",
    "\n",
    "FOR_results_EPIC = earthaccess.search_data(short_name = short_name,\n",
    "                                           temporal = (date_start, date_end), \n",
    "                                           bounding_box = bbox)\n",
    "\n",
    "print(f\"Total number of DSCOVR EPIC L2_AER granules found for TEMPO Field of Regard\"\n",
    "      f\"\\nwithin period of interest between {date_start} and {date_end} is {len(FOR_results_EPIC)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5e603-5fb1-416d-aaf4-34895c4425eb",
   "metadata": {},
   "source": [
    "## 5.1. Ensure all discovered granules have download links\n",
    "\n",
    "Without this step, missing granules can crash the call of `earthaccess.download()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9472c165-a6d7-41ff-8b70-cbd63e44390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805004554_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805015122_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805025649_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805040216_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805050743_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805071838_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805082405_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805092932_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805103500_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805114028_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805124555_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805135123_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805145650_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805160217_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805170745_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805181312_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805191839_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805202406_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805212934_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805223501_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230805234028_03.he5\n"
     ]
    }
   ],
   "source": [
    "def get_url_value_from_result(earthaccess_result):\n",
    "    \"\"\"Return a tuple of the result itself and the data URL, only if the URL field is accessible.\"\"\"\n",
    "    try:\n",
    "        return earthaccess_result, earthaccess_result['umm']['RelatedUrls'][0]['URL']\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Populate the list of results that have links.\n",
    "good_EPIC_result_links = [get_url_value_from_result(result) \n",
    "                           for result in FOR_results_EPIC \n",
    "                           if get_url_value_from_result(result) is not None]\n",
    "\n",
    "# Show the result links\n",
    "for r in sorted(good_EPIC_result_links, key=lambda x: x[1]):\n",
    "    print(r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd38abd-8fef-40e2-b76d-b10b012b42b4",
   "metadata": {},
   "source": [
    "## 5.2. Download DSCOVR EPIC granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "172d7832-8054-4a9c-bbd2-3e2cc2cfb3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224eb80cab8542bca393c235f5a25a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b92ffe705f64b6185f61807d34a858e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4807683e944fca88b19c1dca23144e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downloaded_files = earthaccess.download(FOR_results_EPIC, local_path='.',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8375210-6d33-475c-984b-79b8aad38542",
   "metadata": {},
   "source": [
    "# 6. For every DSCOVR EPIC granule, find simultaneous TEMPO granules and re-map DSCOVR EPIC data to geolocations of TEMPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8ec85-aa8f-4b8f-b474-e6f1401fbf6f",
   "metadata": {},
   "source": [
    "**Cycle through the DSCOVR EPIC granules.**\n",
    "\n",
    "Write re-mapped DSCOVR EPIC UVAI to a netCDF file,\n",
    "and plot the original DSCOVR EPIC and TEMPO along with re-mapped DSCOVR EPIC UVAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38277104-5c8d-481a-babd-01951b138d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epic_granule_link in sorted(good_EPIC_result_links, key=lambda x: x[1]):\n",
    "\n",
    "    dscovr_file_name = epic_granule_link[1][epic_granule_link[1].rfind('/') + 1 : ]\n",
    "    print(dscovr_file_name)\n",
    "\n",
    "    # Read the EPIC Level-2 Aerosol data.\n",
    "    try:\n",
    "        dscovr, dscovr_fv, dscovr_timestamp = read_epic_l2_AER(dscovr_file_name)\n",
    "    except Exception:\n",
    "        print(f\"Unable to find or read hdf5 input granule file {dscovr_file_name}\")\n",
    "        continue\n",
    "\n",
    "    # Search for and download TEMPO data that align with the times of the DSCOVR granule.\n",
    "    tempo_results = get_TEMPO_results_to_match_DSCOVR_granule(dscovr_timestamp)\n",
    "    if tempo_results is None:\n",
    "        continue # if no TEMPO granules found within the DSCOVR EPIC timeframe, go to the next EPIC granule\n",
    "    else:\n",
    "        downloaded_files = earthaccess.download(tempo_results, local_path='.',)\n",
    "\n",
    "    # Generate mask from DSCOVR fill values.\n",
    "    mask = (\n",
    "        (dscovr[\"lat2D\"] != dscovr_fv[\"lat\"]) &\n",
    "        (dscovr[\"lon2D\"] != dscovr_fv[\"lon\"]) &\n",
    "        (dscovr[\"uvai2D\"] != dscovr_fv[\"uvai\"])\n",
    "    )\n",
    "    dscovr_good_latlon = np.column_stack((dscovr[\"lon2D\"][mask], dscovr[\"lat2D\"][mask]))\n",
    "    dscovr_good_uvai2D = dscovr[\"uvai2D\"][mask]\n",
    "\n",
    "    for tempo_result in tempo_results:\n",
    "        tempo_granule_links = tempo_result.data_links()\n",
    "        tempo_file_name = tempo_granule_links[0][tempo_granule_links[0].rfind('/') + 1 : ]\n",
    "\n",
    "        # Read the TEMPO data.\n",
    "        try:\n",
    "            tempo, tempo_fv = read_TEMPO_O3TOT_L2_UVAI(tempo_file_name)\n",
    "        except Exception:\n",
    "            print(f\"TEMPO UVAI cannot be read in file {tempo_file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Generate a polygon representing the boundaries of the TEMPO data.\n",
    "        tempo_polygon = TEMPO_L2_polygon(tempo[\"lat\"], tempo[\"lon\"], tempo_fv[\"geo\"])\n",
    "\n",
    "        # Get positions from TEMPO\n",
    "        tempo_good_lonlat, tempo_extent, lon1D_TEMPO, lat1D_TEMPO, uvai1D_TEMPO = get_good_tempo_points(\n",
    "            tempo[\"lat\"], tempo[\"lon\"], tempo_fv[\"geo\"], tempo[\"uvai\"], tempo_fv[\"uvai\"]\n",
    "        )\n",
    "\n",
    "        # Interpolate DSCOVR data to the TEMPO data locations.\n",
    "        DSCOVR_uvai_at_tempo_points = griddata(\n",
    "            dscovr_good_latlon, dscovr_good_uvai2D, tempo_good_lonlat,\n",
    "            method='linear', fill_value=-999., rescale=False\n",
    "        )\n",
    "\n",
    "        # Determine where the interpolated values fall within the valid range.\n",
    "        valid_mask = (DSCOVR_uvai_at_tempo_points > -30) & (DSCOVR_uvai_at_tempo_points < 30)\n",
    "        if len(DSCOVR_uvai_at_tempo_points[valid_mask]) == 0:\n",
    "            print('no re-mapped DSCOVR pixels within TEMPO granule')\n",
    "            continue\n",
    "        tempo_valid_lon = tempo_good_lonlat[valid_mask, 0]\n",
    "        tempo_valid_lat = tempo_good_lonlat[valid_mask, 1]\n",
    "        DSCOVR_uvai_at_tempo_points_valid = DSCOVR_uvai_at_tempo_points[valid_mask]\n",
    "\n",
    "        # Find where DSCOVR data fall within the boundaries of the TEMPO data.\n",
    "        dscovr_within_tempo_polygon = mask_dscovr_to_tempo(\n",
    "            dscovr[\"lat2D\"], dscovr[\"lon2D\"], dscovr[\"uvai2D\"], dscovr_fv[\"uvai\"], tempo_extent, tempo_polygon\n",
    "        )\n",
    "        if dscovr_within_tempo_polygon is None:\n",
    "            continue  # of no original DSCOVR pixels within TEMPO granule, then go to the next EPIC granule\n",
    "\n",
    "        # Plot comparisons of TEMPO and DSCOVR EPIC UVAI\n",
    "        proj = ccrs.LambertConformal(\n",
    "            central_longitude=(tempo_extent[0] + tempo_extent[1]) * .5,  # -96.0\n",
    "            central_latitude=39.0, \n",
    "            false_easting=0.0, \n",
    "            false_northing=0.0, \n",
    "            standard_parallels=(33, 45), \n",
    "            globe=None, \n",
    "            cutoff=10\n",
    "        )\n",
    "        transform=ccrs.PlateCarree()\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 9), \n",
    "                                dpi=300, facecolor=None, subplot_kw={\"projection\": proj})\n",
    "\n",
    "        for ax in axes.flat:\n",
    "            ax.set_extent(tempo_extent, crs=transform)\n",
    "            ax.coastlines(resolution='50m', color='black', linewidth=1)\n",
    "            grid = ax.gridlines(draw_labels=True, dms=True)\n",
    "            grid.xformatter = LONGITUDE_FORMATTER\n",
    "            grid.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "        # Axis 1\n",
    "        im1 = axes[1].scatter(lon1D_TEMPO, lat1D_TEMPO, c=uvai1D_TEMPO, s=1, cmap=plt.cm.jet,\n",
    "                              vmin=-4., vmax=4., transform=transform)\n",
    "        cb1 = plt.colorbar(im1, ticks=[-4, -2, 0, 2, 4], fraction=0.022, pad=0.01)\n",
    "        cb1.set_label('UVAI', fontsize=10)\n",
    "        axes[1].set_title('UVAI '+ tempo_file_name, size = 10)\n",
    "\n",
    "        # Axis 2\n",
    "        im2 = axes[2].scatter(tempo_valid_lon, tempo_valid_lat,\n",
    "                              c=DSCOVR_uvai_at_tempo_points_valid, s=1, cmap=plt.cm.jet,\n",
    "                              vmin=-4., vmax=4., transform=transform)\n",
    "        cb2 = plt.colorbar(im2, ticks=[-4, -2, 0, 2, 4], fraction=0.022, pad=0.01)\n",
    "        cb2.set_label('UVAI', fontsize=10)\n",
    "        axes[2].set_title('DSCOVR EPIC UVAI re-mapped', size = 10)\n",
    "\n",
    "        # Axis 3\n",
    "        im3 = axes[0].scatter(dscovr_within_tempo_polygon[\"lon1D\"], dscovr_within_tempo_polygon[\"lat1D\"],\n",
    "                              c=dscovr_within_tempo_polygon[\"uvai1D\"], s=1, cmap=plt.cm.jet,\n",
    "                              vmin=-4., vmax=4., transform=transform)\n",
    "        cb3 = plt.colorbar(im3, ticks=[-4, -2, 0, 2, 4], fraction=0.022, pad=0.01)\n",
    "        cb3.set_label('UVAI', fontsize=10)\n",
    "        axes[0].set_title('UVAI '+ dscovr_file_name, size = 10)\n",
    "    \n",
    "        plt.savefig('UVAI_'+ tempo_file_name +'.png', dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48326a76-8b82-4bb1-a532-60b3bc06c60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
